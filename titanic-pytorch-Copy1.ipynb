{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.git', '.ipynb_checkpoints', 'net.pth', 'new_net.pth', 'new_submission.csv', 'README.md', 'submission.csv', 'test.csv', 'titanic-pytorch-Copy1.ipynb', 'titanic-pytorch.ipynb', 'titanic_lime.ipynb', 'train.csv']\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"C:/Users\\Javi\\PycharmProjects\\pythonProject_XAI\\XAI-grupazo\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Se importa la biblioteca PyTorch para trabajar con redes neuronales y tensores.\n",
    "import torch\n",
    "# Se importa el módulo nn de PyTorch, que proporciona las herramientas para definir y entrenar redes neuronales.\n",
    "import torch.nn as nn\n",
    "# Se importa el módulo functional de PyTorch, que contiene varias funciones útiles para trabajar con DNN, como funciones de activación.\n",
    "import torch.nn.functional as F\n",
    "# Se importa la función shuffle del módulo utils de scikit-learn. Esta función se utiliza para mezclar los datos de entrenamiento en cada época.\n",
    "from sklearn.utils import shuffle\n",
    "# Se importa Variable de PyTorch. Se utiliza para envolver los tensores y habilitar el cálculo automático de gradientes durante el entrenamiento.\n",
    "from torch.autograd import Variable\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "f66eed17ca77cd4973b8f00c510c5d98e4dc2af7",
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('train.csv')\n",
    "X_test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "4db62430cd0965249cd85567a0a22a59e56b02ad",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Está creando una lista llamada dataset_title que contiene los títulos de las personas en la columna 'Name' del DataFrame dataset. \n",
    "# Se divide cada nombre en partes con comas y puntos, se toma la segunda parte (índice 1) antes de eliminar los espacios adicionales con strip().\n",
    "dataset_title = [i.split(',')[1].split('.')[0].strip() for i in dataset['Name']]\n",
    "\n",
    "# Crea una nueva columna llamada 'Title' en el DataFrame dataset y la llena con los valores de la lista dataset_title.\n",
    "dataset['Title'] = pd.Series(dataset_title)\n",
    "\n",
    "# Devuelve el recuento de cada valor en la columna 'Title', lo que te da una idea de cuántas veces aparece cada título en el conjunto de datos.\n",
    "dataset['Title'].value_counts()\n",
    "\n",
    "# Reemplaza títulos poco comunes en la columna 'Title' del DataFrame dataset con el valor 'Rare'. \n",
    "# Los títulos específicos que se reemplazan son: \n",
    "# 'Lady', 'the Countess', 'Countess', 'Capt', 'Col', 'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona', 'Ms', 'Mme', 'Mlle'.\n",
    "dataset['Title'] = dataset['Title'].replace(['Lady', 'the Countess', 'Countess', 'Capt', 'Col', 'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona', 'Ms', 'Mme', 'Mlle'], 'Rare')\n",
    "\n",
    "# Las siguientes líneas siguen un proceso similar para el DataFrame X_test, creando la columna 'Title'. \n",
    "# Cuenta los valores únicos y reemplazandolos títulos poco comunes por 'Rare'.\n",
    "dataset_title = [i.split(',')[1].split('.')[0].strip() for i in X_test['Name']]\n",
    "X_test['Title'] = pd.Series(dataset_title)\n",
    "X_test['Title'].value_counts()\n",
    "X_test['Title'] = X_test['Title'].replace(['Lady', 'the Countess', 'Countess', 'Capt', 'Col', 'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona', 'Ms', 'Mme', 'Mlle'], 'Rare')\n",
    "\n",
    "# calcula el tamaño de la familia para cada entrada en el DataFrame dataset. \n",
    "# Suma las columnas 'SibSp' (nº de hermanos/cónyuges a bordo) y 'Parch' (nº de padres/hijos a bordo) y le suma 1 para incluir al pasajero mismo.\n",
    "dataset['FamilyS'] = dataset['SibSp'] + dataset['Parch'] + 1\n",
    "\n",
    "# De manera similar, se calcula el tamaño de la familia para cada entrada en el DataFrame X_test.\n",
    "X_test['FamilyS'] = X_test['SibSp'] + X_test['Parch'] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "06bcd62c208a9f4987b5d9ae182163d1d36e2eae",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# La función family(x) toma un valor x como entrada y asigna una categoría a ese valor en función de las condiciones establecidas:\n",
    "def family(x):\n",
    "    \n",
    "    # Si x es menor que 2, se asigna la categoría 'Single' (soltero).\n",
    "    if x < 2:\n",
    "        return 'Single'\n",
    "    \n",
    "    # Si x es igual a 2, se asigna la categoría 'Couple' (pareja).\n",
    "    elif x == 2:\n",
    "        return 'Couple'\n",
    "    \n",
    "    # Si x es menor o igual a 4, se asigna la categoría 'InterM' (intermedia).\n",
    "    elif x <= 4:\n",
    "        return 'InterM'\n",
    "    \n",
    "    # Si ninguna de las condiciones anteriores se cumple, se asigna la categoría 'Large' (grande).\n",
    "    else:\n",
    "        return 'Large'\n",
    "\n",
    "# Asigna una categoría a cada valor en la columna 'FamilyS' en \"dataset\" y \"X_test\" .\n",
    "# Es una utilidad para analizar el tamaño de la familia y agrupar los datos en categorías más significativas.\n",
    "dataset['FamilyS'] = dataset['FamilyS'].apply(family)\n",
    "X_test['FamilyS'] = X_test['FamilyS'].apply(family)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El código está relacionado con el manejo de valores faltantes (NaN) en diferentes columnas de los DataFrames dataset y X_test. \n",
    "\n",
    "dataset['Embarked'].fillna(dataset['Embarked'].mode()[0], inplace=True): En la columna 'Embarked' del DataFrame dataset, se están \n",
    "reemplazando los valores faltantes con el valor más común (moda) de esa columna. La función mode()[0] devuelve el valor más común y el \n",
    "parámetro inplace=True asegura que los cambios se realicen en el DataFrame original.\n",
    "\n",
    "X_test['Embarked'].fillna(X_test['Embarked'].mode()[0], inplace=True): De manera similar al paso anterior, aquí se reemplazan los valores faltantes\n",
    "en la columna 'Embarked' del DataFrame X_test con la moda de esa columna.\n",
    "\n",
    "dataset['Age'].fillna(dataset['Age'].median(), inplace=True): En la columna 'Age' del DataFrame dataset, se están reemplazando los valores \n",
    "faltantes con la mediana de esa columna. La función median() calcula la mediana y el parámetro inplace=True asegura que los cambios se \n",
    "realicen en el DataFrame original.\n",
    "\n",
    "X_test['Age'].fillna(X_test['Age'].median(), inplace=True): De manera similar al paso anterior, aquí se reemplazan los valores faltantes en la \n",
    "columna 'Age' del DataFrame X_test con la mediana de esa columna.\n",
    "\n",
    "X_test['Fare'].fillna(X_test['Fare'].median(), inplace=True): En la columna 'Fare' del DataFrame X_test, se están reemplazando los valores faltantes\n",
    "con la mediana de esa columna. Esto se realiza solo en el DataFrame X_test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "82dbbb08eadc6bef94dbcf018ee60fdff8288668",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "dataset['Embarked'].fillna(dataset['Embarked'].mode()[0], inplace=True)\n",
    "\n",
    "X_test['Embarked'].fillna(X_test['Embarked'].mode()[0], inplace=True)\n",
    "\n",
    "dataset['Age'].fillna(dataset['Age'].median(), inplace=True)\n",
    "\n",
    "X_test['Age'].fillna(X_test['Age'].median(), inplace=True)\n",
    "\n",
    "X_test['Fare'].fillna(X_test['Fare'].median(), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataset = dataset.drop(['PassengerId', 'Cabin', 'Name', 'SibSp', 'Parch', 'Ticket'], axis=1): En el DataFrame dataset, se están eliminando las \n",
    "columnas 'PassengerId', 'Cabin', 'Name', 'SibSp', 'Parch' y 'Ticket'. El parámetro axis=1 indica que se deben eliminar columnas en lugar de filas.\n",
    "\n",
    "X_test_passengers = X_test['PassengerId']: Se está creando una nueva variable X_test_passengers que contiene los valores de la columna \n",
    "'PassengerId' del DataFrame X_test.\n",
    "\n",
    "X_test = X_test.drop(['PassengerId', 'Cabin', 'Name', 'SibSp', 'Parch', 'Ticket'], axis=1): En el DataFrame X_test, se están eliminando las mismas \n",
    "columnas ('PassengerId', 'Cabin', 'Name', 'SibSp', 'Parch', 'Ticket') que se eliminaron en el DataFrame dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "620ce83ee4ff808ea787ef08583575ef15d63888",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "dataset = dataset.drop(['PassengerId', 'Cabin', 'Name', 'SibSp', 'Parch', 'Ticket'], axis=1)\n",
    "\n",
    "X_test_passengers = X_test['PassengerId']\n",
    "\n",
    "X_test = X_test.drop(['PassengerId', 'Cabin', 'Name', 'SibSp', 'Parch', 'Ticket'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>Title</th>\n",
       "      <th>FamilyS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>S</td>\n",
       "      <td>Mr</td>\n",
       "      <td>Couple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C</td>\n",
       "      <td>Mrs</td>\n",
       "      <td>Couple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>S</td>\n",
       "      <td>Miss</td>\n",
       "      <td>Single</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>S</td>\n",
       "      <td>Mrs</td>\n",
       "      <td>Couple</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>S</td>\n",
       "      <td>Mr</td>\n",
       "      <td>Single</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Survived  Pclass     Sex   Age     Fare Embarked Title FamilyS\n",
       "0         0       3    male  22.0   7.2500        S    Mr  Couple\n",
       "1         1       1  female  38.0  71.2833        C   Mrs  Couple\n",
       "2         1       3  female  26.0   7.9250        S  Miss  Single\n",
       "3         1       1  female  35.0  53.1000        S   Mrs  Couple\n",
       "4         0       3    male  35.0   8.0500        S    Mr  Single"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X_train = dataset.iloc[:, 1:9].values: Aquí se está extrayendo una porción de dataset que incluye todas las filas y las columnas de la 1 a la 9 \n",
    "(excluyendo la columna 0). Los valores se asignan a la variable X_train.\n",
    "\n",
    "Y_train = dataset.iloc[:, 0].values: Se extraen los valores de la primera columna (columna 0) de dataset y se asignan a la variable Y_train. \n",
    "Esto se hace porque la columna 0 contiene la variable objetivo (la etiqueta o resultado que se desea predecir).\n",
    "\n",
    "X_test = X_test.values: Aquí se extraen los valores del DataFrame X_test y se asignan a la variable X_test. Esto se hace para preparar los datos \n",
    "de prueba para su uso posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "ef54d377d5abe31de761a2543c373b0c510ae401",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "X_train = dataset.iloc[:, 1:9].values\n",
    "\n",
    "Y_train = dataset.iloc[:, 0].values\n",
    "\n",
    "X_test = X_test.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "labelencoder_X_1 = LabelEncoder(): Se crea una instancia de LabelEncoder y se asigna a la variable labelencoder_X_1.\n",
    "\n",
    "X_train[:, 1] = labelencoder_X_1.fit_transform(X_train[:, 1]): La columna 1 de X_train (la cual contiene etiquetas categóricas) se convierte a valores\n",
    "numéricos utilizando el labelencoder_X_1. Los valores convertidos se asignan de nuevo a la columna 1 de X_train.\n",
    "\n",
    "En los siguientes pasos se repite para las columnas 4, 5 y 6 de X_train.\n",
    "\n",
    "labelencoder_X_2 = LabelEncoder(): Se crea otra instancia de LabelEncoder y se asigna a la variable labelencoder_X_2.\n",
    "\n",
    "Se aplican idénticos cambios a las columnas correspondientes de X_test en lugar de X_train.\n",
    "\n",
    "\n",
    "Este código utiliza LabelEncoder para convertir las etiquetas categóricas en las columnas seleccionadas de X_train y X_test en valores numéricos \n",
    "para que los algoritmos de aprendizaje automático puedan trabajar con características categóricas en lugar de texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "813ccdbc8e16e80a7401157e56523a5c75c534f0",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Converting the remaining labels to numbers\n",
    "labelencoder_X_1 = LabelEncoder()\n",
    "X_train[:, 1] = labelencoder_X_1.fit_transform(X_train[:, 1])\n",
    "X_train[:, 4] = labelencoder_X_1.fit_transform(X_train[:, 4])\n",
    "X_train[:, 5] = labelencoder_X_1.fit_transform(X_train[:, 5])\n",
    "X_train[:, 6] = labelencoder_X_1.fit_transform(X_train[:, 6])\n",
    "\n",
    "labelencoder_X_2 = LabelEncoder()\n",
    "X_test[:, 1] = labelencoder_X_2.fit_transform(X_test[:, 1])\n",
    "X_test[:, 4] = labelencoder_X_2.fit_transform(X_test[:, 4])\n",
    "X_test[:, 5] = labelencoder_X_2.fit_transform(X_test[:, 5])\n",
    "X_test[:, 6] = labelencoder_X_2.fit_transform(X_test[:, 6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Índices de las columnas categóricas\n",
    "categorical_columns = [0, 1, 4, 5, 6]\n",
    "\n",
    "# Crear el transformador de columnas\n",
    "column_transformer = ColumnTransformer(\n",
    "    [('one_hot_encoder', OneHotEncoder(), categorical_columns)],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# Aplicar la transformación a X_train y X_test\n",
    "X_train = column_transformer.fit_transform(X_train)\n",
    "X_test = column_transformer.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.astype(float)\n",
    "X_test = X_test.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "4444e870e198520bbbd400b780cafb88571feb8b",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Se utiliza la función train_test_split de scikit-learn para dividir los datos de entrenamiento (X_train y Y_train) \n",
    "en conjuntos de entrenamiento y validación.\n",
    "'''\n",
    "x_train, x_val, y_train, y_val = train_test_split(X_train, Y_train, test_size = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(801, 19) (90, 19) (801,) (90,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape, x_val.shape, y_train.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "cd935e70f6d2185b65e478210381819df938cca7",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "En este código se define una red neuronal utilizando la biblioteca PyTorch.\n",
    "'''\n",
    "\n",
    "# Se define una clase llamada Net que hereda de la clase nn.Module, que es la base para definir redes neuronales en PyTorch.\n",
    "class Net(nn.Module):\n",
    "    \n",
    "    # Se define el método constructor, que se llama al crear una instancia de la clase Net. Aquí se definen las capas de la red neuronal.\n",
    "    def __init__(self):\n",
    "        \n",
    "        # super(Net, self).__init__(): Se llama al método __init__ de la clase base nn.Module para inicializar la clase.\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        # Se define la capa completamente conectada fc1 con 19 entradas y 270 neuronas.\n",
    "        self.fc1 = nn.Linear(19, 270)\n",
    "        \n",
    "        # Se define la capa completamente conectada fc2 con 270 entradas y 2 neuronas, que corresponde a las clases de salida.\n",
    "        self.fc2 = nn.Linear(270, 2)\n",
    "        \n",
    "    # Se define el método forward, que especifica cómo los datos se propagan a través de la red neuronal.\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Los datos de entrada x se propagan a través de la capa fc1.\n",
    "        x = self.fc1(x)\n",
    "        \n",
    "        # Se aplica una capa de dropout con una probabilidad de desactivación del 10% a los datos x.\n",
    "        # Esto ayuda a prevenir el sobreajuste en la red neuronal.\n",
    "        x = F.dropout(x, p=0.1)\n",
    "        \n",
    "        # Se aplica la función de activación ReLU a los datos x.\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        # Los datos x se propagan a través de la capa fc2.\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        # Se aplica la función de activación sigmoide a los datos x para obtener las probabilidades de clase.\n",
    "\n",
    "# C:\\Users\\Javi\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. \n",
    "# Use torch.sigmoid instead. warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.)\n",
    "        x = torch.softmax(x, dim=1)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "    def predict(self, x):\n",
    "        \n",
    "        if isinstance(x, np.ndarray):\n",
    "            \n",
    "            x = torch.from_numpy(x).float()\n",
    "            \n",
    "        elif isinstance(x, torch.Tensor):\n",
    "            \n",
    "            x = x.float()\n",
    "\n",
    "        # Asegúrate de que los datos de entrada tengan la forma correcta\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        # Propagación hacia adelante de la red neuronal\n",
    "        output = self.forward(x)\n",
    "\n",
    "        # Convierte las probabilidades de clase a etiquetas (0 o 1)\n",
    "        _, predicted_labels = torch.max(output, 1)\n",
    "    \n",
    "        return predicted_labels.numpy()\n",
    "\n",
    "# Se crea una instancia de la clase Net y se asigna a la variable net.\n",
    "new_net = Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'   \\n    def predict(self, x):\\n        \\n        \\n        # Aplica la propagación hacia adelante de la red neuronal\\n        output = self.forward(x)\\n        # Convierte las probabilidades de clase a etiquetas (0 o 1)\\n        _, predicted_labels = torch.max(output, 1)\\n        return predicted_labels\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''   \n",
    "    def predict(self, x):\n",
    "        \n",
    "        \n",
    "        # Aplica la propagación hacia adelante de la red neuronal\n",
    "        output = self.forward(x)\n",
    "        # Convierte las probabilidades de clase a etiquetas (0 o 1)\n",
    "        _, predicted_labels = torch.max(output, 1)\n",
    "        return predicted_labels\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "9d5641b90eccab8e9643e288c767984a539d63e4",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "batch_size = 50: Se define el tamaño del lote, que es el número de muestras que se utilizan para calcular los gradientes y actualizar los pesos en\n",
    "cada paso de entrenamiento. En este caso, se establece en 50, lo que significa que se utilizarán 50 muestras a la vez durante el entrenamiento.\n",
    "\n",
    "num_epochs = 50: Se define el número de épocas, que es la cantidad de veces que el modelo pasará por todo el conjunto de entrenamiento.\n",
    "En este caso, se establece en 50, lo que significa que el modelo se entrenará durante 50 pasadas completas por el conjunto de entrenamiento.\n",
    "\n",
    "learning_rate = 0.01: Se define la tasa de aprendizaje, que controla el tamaño de los pasos de actualización de los pesos durante el \n",
    "entrenamiento. En este caso, se establece en 0.01, lo que significa que los pesos se actualizarán en incrementos pequeños.\n",
    "\n",
    "batch_no = len(x_train) // batch_size: Se calcula el número de lotes en cada época dividiendo el número total de muestras de entrenamiento \n",
    "(len(x_train)) por el tamaño del lote (batch_size). Esto proporciona el número de veces que se iterará sobre los lotes durante cada época.\n",
    "'''\n",
    "\n",
    "batch_size = 25\n",
    "num_epochs = 100\n",
    "learning_rate = 0.001\n",
    "batch_no = len(x_train) // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_uuid": "938adf17cd24b2754282ca060cfb64a9df87c102",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "criterion = nn.CrossEntropyLoss(): Se crea una instancia de la clase CrossEntropyLoss de PyTorch. Esta función de pérdida se utiliza para \n",
    "problemas de clasificación multiclase. En este caso, se utiliza para calcular la pérdida entre las probabilidades de salida de la red neuronal y las\n",
    "etiquetas verdaderas.\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate): Se crea una instancia del optimizador Adam utilizando los parámetros de la red \n",
    "neuronal (net.parameters()) y la tasa de aprendizaje (learning_rate). El optimizador Adam es un algoritmo de optimización popular utilizado para \n",
    "ajustar los pesos de la red neuronal durante el entrenamiento. Se encarga de actualizar los pesos de acuerdo con los gradientes calculados a \n",
    "partir de la función de pérdida.\n",
    "'''\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(new_net.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "9f2d8485cc5aac6b03a1aca4c163b5c6f5cd4259"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Epoch 6\n",
      "Epoch 11\n",
      "Epoch 16\n",
      "Epoch 21\n",
      "Epoch 26\n",
      "Epoch 31\n",
      "Epoch 36\n",
      "Epoch 41\n",
      "Epoch 46\n",
      "Epoch 51\n",
      "Epoch 56\n",
      "Epoch 61\n",
      "Epoch 66\n",
      "Epoch 71\n",
      "Epoch 76\n",
      "Epoch 81\n",
      "Epoch 86\n",
      "Epoch 91\n",
      "Epoch 96\n"
     ]
    }
   ],
   "source": [
    "#  Itera sobre el número de épocas definido anteriormente. En cada época, se entrena la red neuronal.\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    # Se imprime un mensaje cada 5 épocas para mostrar el progreso del entrenamiento.\n",
    "    if epoch % 5 == 0:\n",
    "        print('Epoch {}'.format(epoch+1))\n",
    "        \n",
    "    # Los datos de entrenamiento (x_train y y_train) se mezclan utilizando la función shuffle de scikit-learn. \n",
    "    # Esto garantiza que los datos se presenten en un orden aleatorio durante el entrenamiento.    \n",
    "    x_train, y_train = shuffle(x_train, y_train)\n",
    "    # Mini batch learning\n",
    "    \n",
    "    # Itera sobre el número de lotes en cada época.\n",
    "    for i in range(batch_no):\n",
    "        \n",
    "        # Se calcula el índice de inicio del lote actual.\n",
    "        start = i * batch_size\n",
    "        \n",
    "        # Se calcula el índice de fin del lote actual.  \n",
    "        end = start + batch_size\n",
    "        \n",
    "        # Se crea una variable x_var envolviendo el lote actual de características de entrenamiento en un tensor de punto flotante.\n",
    "        x_var = Variable(torch.FloatTensor(x_train[start:end]))\n",
    "        \n",
    "        # Se crea una variable y_var envolviendo el lote actual de etiquetas de entrenamiento en un tensor de números enteros largos.\n",
    "        y_var = Variable(torch.LongTensor(y_train[start:end]))\n",
    "        \n",
    "        # Se borran los gradientes acumulados de los parámetros del modelo antes de calcular los nuevos gradientes.\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Se realiza una pasada hacia adelante de los datos de entrada x_var a través de la red neuronal para obtener las predicciones ypred_var.\n",
    "        ypred_var = new_net(x_var)\n",
    "        \n",
    "        # Se calcula la pérdida utilizando la función de pérdida criterion entre las predicciones ypred_var y las etiquetas y_var.\n",
    "        loss =criterion(ypred_var, y_var)\n",
    "        \n",
    "        # Se realiza la retropropagación de la pérdida para calcular los gradientes de los parámetros de la red neuronal.\n",
    "        loss.backward()\n",
    "        \n",
    "        # Se realiza un paso de optimización para actualizar los pesos de la red neuronal basados en los gradientes calculados.\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_uuid": "b30b4c578b3ddd40ec9f63e19396b045c5ac0dd5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.83\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "test_var = Variable(torch.FloatTensor(x_val), requires_grad=True)\n",
    "with torch.no_grad():\n",
    "    result = new_net(test_var)\n",
    "values, labels = torch.max(result, 1)\n",
    "num_right = np.sum(labels.data.numpy() == y_val)\n",
    "print('Accuracy {:.2f}'.format(num_right / len(y_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_uuid": "10054399a4160803528137e2ebf60b73394b6007"
   },
   "outputs": [],
   "source": [
    "# Applying model on the test data\n",
    "X_test_var = Variable(torch.FloatTensor(X_test), requires_grad=True) \n",
    "with torch.no_grad():\n",
    "    test_result = new_net(X_test_var)\n",
    "values, labels = torch.max(test_result, 1)\n",
    "survived = labels.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_uuid": "901d7d529bfe936d8c5091e2a73ffc9d1c5da8ed",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "new_submission = [['PassengerId', 'Survived']]\n",
    "for i in range(len(survived)):\n",
    "    new_submission.append([X_test_passengers[i], survived[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "_uuid": "21b294d4940218f5d2bef3782aca9a6ba6e9c054"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing Complete!\n"
     ]
    }
   ],
   "source": [
    "with open('new_submission.csv', 'w') as submissionFile:\n",
    "    writer = csv.writer(submissionFile)\n",
    "    writer.writerows(new_submission)\n",
    "    \n",
    "print('Writing Complete!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "_uuid": "84c1c29464abb99948a1ab01ef64ef33f72c90d5",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "new_submission = pd.read_csv('new_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_submission = pd.DataFrame(new_submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>892</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>893</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>894</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>895</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>896</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>413</th>\n",
       "      <td>1305</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>414</th>\n",
       "      <td>1306</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>415</th>\n",
       "      <td>1307</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>416</th>\n",
       "      <td>1308</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>1309</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>418 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId  Survived\n",
       "0            892         0\n",
       "1            893         0\n",
       "2            894         0\n",
       "3            895         0\n",
       "4            896         1\n",
       "..           ...       ...\n",
       "413         1305         0\n",
       "414         1306         1\n",
       "415         1307         0\n",
       "416         1308         0\n",
       "417         1309         1\n",
       "\n",
       "[418 rows x 2 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(new_net.state_dict(), 'new_net.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19,)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[100].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n"
     ]
    }
   ],
   "source": [
    "# Ejemplo de datos de entrada\n",
    "input_data = torch.tensor(X_train[10]).float()\n",
    "\n",
    "# Asegúrate de que los datos de entrada tengan la forma correcta\n",
    "input_data = input_data.view(1, -1)\n",
    "\n",
    "predicted_labels = new_net.predict(input_data)\n",
    "\n",
    "# Imprime las etiquetas predichas\n",
    "print(predicted_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 1 1 0 0 0 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "# Supongamos que tienes un tensor input_data de tamaño (n_samples, n_features) con las muestras de entrada\n",
    "input_data = torch.tensor(X_train[:10])  # Muestra 3\n",
    "\n",
    "# Asegúrate de que los datos de entrada tengan la forma correcta\n",
    "input_data = input_data.float()\n",
    "\n",
    "# Obtén el número de muestras y características\n",
    "n_samples, n_features = input_data.shape\n",
    "\n",
    "# Cambia la forma de los datos de entrada para que sea (n_samples, n_features)\n",
    "input_data = input_data.view(n_samples, n_features)\n",
    "\n",
    "# Realiza las predicciones en el lote de muestras\n",
    "predicted_labels = new_net.predict(input_data)\n",
    "\n",
    "# Imprime las etiquetas predichas\n",
    "print(predicted_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
